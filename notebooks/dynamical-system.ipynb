{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Can text accumulation be modeled as a dynamical system?\n",
    "\n",
    "There are several inferential leaps in the original paper that \"feel right\" but are strictly empirical observations. For example, defining higher $n$-legomena counts in terms of $n$th partial derivatives of the type count formula. This was inferred by looking at the equations and checked against data. But what is the intuition behind it? How do hapax counts relate to the first derivative of types? Can this be motivated in plain English, by modeling text accumulation as a dynamical system, described in the language of differential equations, whose solutions are those formulae described in the paper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bloody dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import gutenberg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# custom classes\n",
    "from legomena import Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Book\n",
    "\n",
    "We're going to do something a little different this time. Let's take a small example to be able to work with hands-on. For fun, let's take the first paragraph of Moby Dick: Call me Ishmael..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Call', 'me', 'Ishmael']\n"
     ]
    }
   ],
   "source": [
    "# moby dick\n",
    "words = gutenberg.words(\"melville-moby_dick.txt\")\n",
    "\n",
    "# play example: paragraph 1\n",
    "words = list(words)[4712:4940]\n",
    "print(words[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Accumulation\n",
    "\n",
    "Let's simulate the accumulation of text by literally building a corpus of each snapshot of the book's opening: the WFD of the first word, the first two words, the first three words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Call</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>me</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ishmael</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type  freq\n",
       "rank               \n",
       "1        Call     1\n",
       "2          me     1\n",
       "3     Ishmael     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of corpuses\n",
    "corpi = [ Corpus(words[:n]) for n in range(len(words)) ]\n",
    "total = corpi[-1]\n",
    "\n",
    "# first three words\n",
    "corpi[3].WFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9., 111.,   9.,   2.,   7.,   2.,   0.,   2.,   1.,   2.,   0.],\n",
       "       [  9., 110.,  10.,   2.,   7.,   2.,   0.,   2.,   1.,   2.,   0.],\n",
       "       [  8., 111.,  10.,   2.,   7.,   2.,   0.,   2.,   1.,   2.,   0.],\n",
       "       [  8., 111.,  10.,   2.,   7.,   2.,   0.,   2.,   1.,   1.,   1.],\n",
       "       [  7., 112.,  10.,   2.,   7.,   2.,   0.,   2.,   1.,   1.,   1.],\n",
       "       [  6., 113.,  10.,   2.,   7.,   2.,   0.,   2.,   1.,   1.,   1.],\n",
       "       [  5., 114.,  10.,   2.,   7.,   2.,   0.,   2.,   1.,   1.,   1.],\n",
       "       [  5., 114.,  10.,   2.,   7.,   2.,   0.,   2.,   0.,   2.,   1.],\n",
       "       [  4., 115.,  10.,   2.,   7.,   2.,   0.,   2.,   0.,   2.,   1.],\n",
       "       [  3., 116.,  10.,   2.,   7.,   2.,   0.,   2.,   0.,   2.,   1.],\n",
       "       [  2., 117.,  10.,   2.,   7.,   2.,   0.,   2.,   0.,   2.,   1.],\n",
       "       [  2., 117.,  10.,   2.,   7.,   2.,   0.,   2.,   0.,   1.,   2.],\n",
       "       [  1., 118.,  10.,   2.,   7.,   2.,   0.,   2.,   0.,   1.,   2.],\n",
       "       [  0., 119.,  10.,   2.,   7.,   2.,   0.,   2.,   0.,   1.,   2.],\n",
       "       [  0., 119.,  10.,   2.,   6.,   3.,   0.,   2.,   0.,   1.,   2.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
       "       [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.],\n",
       "       [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
       "       [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'00000000000001000000000000001001001000011000001010010201002020302030000140000120200110114110000033005020050240220000003240103000103000060550000001206602002130001703023000604040000000038004705031020600003700030080110900080009004'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k vectors\n",
    "L = len(total.k)\n",
    "N = sum(total.k)\n",
    "kvecs = []\n",
    "for corpus in corpi:\n",
    "    kvec = corpus.k\n",
    "    kvec.resize(L)\n",
    "    kvec[0] = N - kvec.sum()\n",
    "    kvecs.append(kvec)\n",
    "kvecs = np.array(kvecs)\n",
    "display(kvecs[-15:,:])\n",
    "\n",
    "# delta vectors\n",
    "deltas = kvecs[1:,:] - kvecs[:-1, :]\n",
    "display(deltas[-15:,:])\n",
    "\n",
    "# choice indexes\n",
    "idx = [ np.argwhere(delta<0)[0][0] for delta in deltas ]\n",
    "\"\".join([str(i) for i in idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have here the empirical data embodying the phenomenon of text accumulation. At any given time, you have a bag of words $W$ describable by a word frequency distribution $\\langle W \\rangle$, itself describable as a distribution over those counts $\\langle \\langle W \\rangle \\rangle$. In the example above, we have $M=228$ tokens of $N=145$ types, $H=119$ of which are hapaxes, that is, tokens that appear exactly once. Sticking with our established notation, we use a zero-based vector $\\hat{k}(x)$ to denote the evolution of $\\langle \\langle W \\rangle \\rangle$ as a function of sample size $x \\in [0,1]$. Thus, $M=\\sum{nk_n(x)}$ and $N=\\sum{k_n(x)}$ for all $x$. Note that the zeroth component allows us to track the \"number of types _not_ sampled\". Therefore, if we're being particular about the summary indexes, then $N=\\sum_{n=0}^\\infty k_n(x)$ and $E(x)=\\sum_{n=1}^\\infty k_n(x) = N - k_0(x)$.\n",
    "\n",
    "This starting notation in hand, what happens when we're in state $\\hat{k}$ and draw one new token? For starters, _something_ must change. Either an unknown word is drawn which becomes a hapax, so $k_0$ decreases by one and $k_1$ increases by one. Or a known word is drawn. If this known word repeated in the text $n$ times, then now it repeats $n+1$ times, so $k_n$ decreases by one and $k_{n+1}$ increases by one. What we've described here is a Markov process describable as state $\\hat{k}$ with legal transitions $n \\to n+1$, \"choose a token which repeated $n$ times and now thus repeats $n+1$ times\" for $n \\ge 0$. This dramatically reduces the dimension of the problem, by realizing this is the _only_ step transition possible.\n",
    "\n",
    "Denote the probability of each transition by $\\delta_n(x) = Pr(n \\to n+1)$ at sample size $x$. And you now have expressions for the _change in $\\hat{k}$_ over time:\n",
    "\n",
    "$$\n",
    "\\hat{k}'(x) = \\langle k_0'(x), k_1'(x), k_2'(x), k_3'(x), ...\\rangle \\\\\n",
    "k_0'(x) = -\\delta_0(x) \\\\\n",
    "k_n'(x) = -\\delta_{n-1} + \\delta_n(x)\n",
    "$$\n",
    "\n",
    "So, what _is_ the probability of each transition? Well, the total number of _tokens_ in the system is $M$. At sample fraction $x$ that gives $xM$ tokens drawn and $(1-x)M$ tokens undrawn. The number of tokens in \"bin\" $n$ at time $x$ is $nk_n(x)$ for $n>0$ and $(1-x)M$ for $n=0$, so the probability of drawing a token from bin $n$ is $\\frac{1}{M}nk_n(x)$ for $n>0$ and $1-x$ for $n=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr(0->1) = 0.5638766519823788\n",
      "Pr(n->n+1) = [0.56387665 0.2246696  0.10572687 0.02643172 0.03524229 0.04405286\n",
      " 0.         0.         0.         0.         0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of transition\n",
    "M = total.M\n",
    "m = 99\n",
    "nvec = np.array(range(L))\n",
    "kvec = kvecs[m]\n",
    "delta = nvec * kvec / M\n",
    "delta[0] = 1-m/M\n",
    "\n",
    "# display\n",
    "print(\"Pr(0->1) =\", delta[0])\n",
    "print(\"Pr(n->n+1) =\", delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
